{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Uv4Yy46gzd6",
    "outputId": "aa7c4fce-a303-4dcc-c001-2fa444857bf4"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/kailas93/Tree_species.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vOjkUhcviFvU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  CSV files are Git LFS pointers, not actual data!\n",
      "üì• Please download real data from: https://datadryad.org/stash/dataset/doi:10.5061/dryad.2rbnzs7hs\n",
      "üîÑ Creating sample data for demonstration...\n",
      "‚úÖ Created sample dataset with 10,000 records\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "# # # Unzip the dataset\n",
    "# zip_path = \"/content/archive(1).zip\"\n",
    "extract_dir = \"5M_trees\"\n",
    "\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Define which columns to keep\n",
    "selected_columns = [\n",
    "    'common_name', 'scientific_name', 'city', 'state',\n",
    "    'longitude_coordinate', 'latitude_coordinate', 'address', 'condition',\n",
    "    'native', 'height_binned_M', 'diameter_breast_height_binned_CM',\n",
    "    'location_type', 'zipcode', 'neighborhood', 'location_name', 'ward',\n",
    "    'district', 'overhead_utility', 'diameter_breast_height_CM', 'height_M'\n",
    "]\n",
    "\n",
    "# Check if CSV files contain actual data or are Git LFS pointers\n",
    "exclude_files = {'Column_Headers_Dryad.csv', 'README_Dryad.txt'}\n",
    "csv_files = [f for f in glob.glob(os.path.join(extract_dir, \"*.csv\")) if os.path.basename(f) not in exclude_files]\n",
    "\n",
    "# Check if we have real data\n",
    "if csv_files:\n",
    "    sample_file = csv_files[0]\n",
    "    with open(sample_file, 'r') as f:\n",
    "        first_line = f.readline().strip()\n",
    "    \n",
    "    if \"git-lfs\" in first_line:\n",
    "        print(\"‚ö†Ô∏è  CSV files are Git LFS pointers, not actual data!\")\n",
    "        print(\"üì• Please download real data from: https://datadryad.org/stash/dataset/doi:10.5061/dryad.2rbnzs7hs\")\n",
    "        print(\"üîÑ Creating sample data for demonstration...\")\n",
    "        \n",
    "        # Create sample data for demonstration\n",
    "        np.random.seed(42)\n",
    "        n_samples = 10000\n",
    "        \n",
    "        tree_species = ['Oak', 'Maple', 'Pine', 'Birch', 'Cedar', 'Elm', 'Willow', 'Poplar', 'Ash', 'Cherry']\n",
    "        cities = ['Austin', 'Dallas', 'Houston', 'Seattle', 'Portland', 'Denver', 'Phoenix', 'Chicago', 'Boston', 'New York']\n",
    "        states = ['TX', 'WA', 'OR', 'CO', 'AZ', 'IL', 'MA', 'NY']\n",
    "        \n",
    "        sample_data = {\n",
    "            'common_name': np.random.choice(tree_species, n_samples),\n",
    "            'scientific_name': [f\"Species_{i}\" for i in range(n_samples)],\n",
    "            'city': np.random.choice(cities, n_samples),\n",
    "            'state': np.random.choice(states, n_samples),\n",
    "            'longitude_coordinate': np.random.uniform(-125.0, -67.0, n_samples),\n",
    "            'latitude_coordinate': np.random.uniform(25.0, 49.0, n_samples),\n",
    "            'address': [f\"Address_{i}\" for i in range(n_samples)],\n",
    "            'condition': np.random.choice(['Good', 'Fair', 'Poor'], n_samples),\n",
    "            'native': np.random.choice(['Native', 'Non-native'], n_samples),\n",
    "            'height_binned_M': np.random.uniform(1.0, 30.0, n_samples),\n",
    "            'diameter_breast_height_binned_CM': np.random.uniform(5.0, 100.0, n_samples),\n",
    "            'location_type': np.random.choice(['Street', 'Park', 'Yard'], n_samples),\n",
    "            'zipcode': np.random.randint(10000, 99999, n_samples),\n",
    "            'neighborhood': [f\"Neighborhood_{i%50}\" for i in range(n_samples)],\n",
    "            'location_name': [f\"Location_{i}\" for i in range(n_samples)],\n",
    "            'ward': np.random.randint(1, 20, n_samples),\n",
    "            'district': np.random.randint(1, 10, n_samples),\n",
    "            'overhead_utility': np.random.choice(['Yes', 'No'], n_samples),\n",
    "            'diameter_breast_height_CM': np.random.uniform(5.0, 100.0, n_samples),\n",
    "            'height_M': np.random.uniform(1.0, 30.0, n_samples)\n",
    "        }\n",
    "        \n",
    "        merged_df = pd.DataFrame(sample_data)\n",
    "        print(f\"‚úÖ Created sample dataset with {len(merged_df):,} records\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚úÖ Found real CSV data files!\")\n",
    "        # Original code for processing real CSV files\n",
    "        df_list = []\n",
    "        for file in csv_files:\n",
    "            df = pd.read_csv(file, low_memory=False)\n",
    "            filtered_df = df[selected_columns].copy()\n",
    "            df_list.append(filtered_df)\n",
    "\n",
    "        merged_df = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"‚úÖ Processed {len(csv_files)} CSV files with {len(merged_df):,} records\")\n",
    "else:\n",
    "    print(\"‚ùå No CSV files found!\")\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "# Add tree_id column\n",
    "if not merged_df.empty:\n",
    "    merged_df.insert(0, 'tree_id', ['tree_' + str(i) for i in range(1, len(merged_df) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "xOwn5LQXiiRo",
    "outputId": "b3cf048b-c3f1-4c0e-ab58-4a94ca85c445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tree_id                             0\n",
       "common_name                         0\n",
       "scientific_name                     0\n",
       "city                                0\n",
       "state                               0\n",
       "longitude_coordinate                0\n",
       "latitude_coordinate                 0\n",
       "address                             0\n",
       "condition                           0\n",
       "native                              0\n",
       "height_binned_M                     0\n",
       "diameter_breast_height_binned_CM    0\n",
       "location_type                       0\n",
       "zipcode                             0\n",
       "neighborhood                        0\n",
       "location_name                       0\n",
       "ward                                0\n",
       "district                            0\n",
       "overhead_utility                    0\n",
       "diameter_breast_height_CM           0\n",
       "height_M                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tFLJ_NJVit4M"
   },
   "outputs": [],
   "source": [
    "# Drop columns with more than 3,038,500 missing values\n",
    "threshold = 3038501\n",
    "merged_df = merged_df.loc[:, merged_df.isnull().sum() <= threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "HX4JWJt0jHhd",
    "outputId": "ee7cdb35-942d-4b1e-9dab-625da095a4f4"
   },
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['diameter_breast_height_binned_CM'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1KZbvRiliwT4"
   },
   "outputs": [],
   "source": [
    "merged_df = merged_df.dropna(subset=[\n",
    "    'common_name',\n",
    "    'scientific_name',\n",
    "    'longitude_coordinate',\n",
    "    'latitude_coordinate',\n",
    "    'condition',\n",
    "    'diameter_breast_height_CM','address', 'city'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "wO-S8Swii7mz",
    "outputId": "382dd608-42c1-4816-a914-e5d19125f007"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tree_id                      0\n",
       "common_name                  0\n",
       "scientific_name              0\n",
       "city                         0\n",
       "state                        0\n",
       "longitude_coordinate         0\n",
       "latitude_coordinate          0\n",
       "address                      0\n",
       "condition                    0\n",
       "native                       0\n",
       "height_binned_M              0\n",
       "location_type                0\n",
       "zipcode                      0\n",
       "neighborhood                 0\n",
       "location_name                0\n",
       "ward                         0\n",
       "district                     0\n",
       "overhead_utility             0\n",
       "diameter_breast_height_CM    0\n",
       "height_M                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wtlKsJKzjADx"
   },
   "outputs": [],
   "source": [
    "# Step 0: Remove tree species with < 2 samples\n",
    "species_counts = merged_df['common_name'].value_counts()\n",
    "valid_species = species_counts[species_counts >= 3].index.tolist()\n",
    "\n",
    "# Keep only valid species\n",
    "filtered_df = merged_df[merged_df['common_name'].isin(valid_species)].copy()\n",
    "\n",
    "# Verify the filtering worked\n",
    "assert filtered_df['common_name'].value_counts().min() >= 3, \"Still has species with < 2 samples!\"\n",
    "\n",
    "# Continue with filtered data\n",
    "data = filtered_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "I3PYlq0-kz8P",
    "outputId": "c0d55f4f-6985-4a15-d288-679f331a587e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "common_name\n",
       "Oak       1053\n",
       "Cherry    1034\n",
       "Elm       1021\n",
       "Willow    1017\n",
       "Pine       996\n",
       "Ash        994\n",
       "Maple      985\n",
       "Birch      971\n",
       "Poplar     967\n",
       "Cedar      962\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['common_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oivLq3FGkuAx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0bK-BagHjM_R"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "# Load data\n",
    "df = filtered_df.copy()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "# Load data\n",
    "df = merged_df.copy()\n",
    "\n",
    "# Optional: Simplify to genus\n",
    "df['genus'] = df['scientific_name'].apply(lambda x: x.split()[0])\n",
    "\n",
    "# Encode categorical variables (native, city, state)\n",
    "df['native_encoded'] = df['native'].astype('category').cat.codes\n",
    "df['city_encoded'] = df['city'].astype('category').cat.codes\n",
    "df['state_encoded'] = df['state'].astype('category').cat.codes\n",
    "\n",
    "# Features to use\n",
    "feature_cols = ['latitude_coordinate', 'longitude_coordinate', 'diameter_breast_height_CM',\n",
    "                'native_encoded', 'city_encoded', 'state_encoded']\n",
    "X = df[feature_cols]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit Nearest Neighbors model\n",
    "nn_model = NearestNeighbors(n_neighbors=50, algorithm='ball_tree')  # can tune n_neighbors\n",
    "nn_model.fit(X_scaled)\n",
    "\n",
    "# Prediction function\n",
    "def recommend_species(lat, lon, diameter_cm, native, city, state, top_n=5):\n",
    "    # Encode input\n",
    "    native_code = df['native'].astype('category').cat.categories.get_loc(native)\n",
    "    city_code = df['city'].astype('category').cat.categories.get_loc(city)\n",
    "    state_code = df['state'].astype('category').cat.categories.get_loc(state)\n",
    "\n",
    "    input_features = np.array([[lat, lon, diameter_cm, native_code, city_code, state_code]])\n",
    "    input_scaled = scaler.transform(input_features)\n",
    "\n",
    "    distances, indices = nn_model.kneighbors(input_scaled)\n",
    "\n",
    "    # Get common names or genera from neighbors\n",
    "    neighbors = df.iloc[indices[0]]\n",
    "    species_counts = Counter(neighbors['common_name'])  # or use 'genus'\n",
    "\n",
    "    # Top-N species\n",
    "    top_species = species_counts.most_common(top_n)\n",
    "    return top_species\n",
    "# Optional: Simplify to genus\n",
    "df['genus'] = df['scientific_name'].apply(lambda x: x.split()[0])\n",
    "\n",
    "# Encode categorical variables (native, city, state)\n",
    "df['native_encoded'] = df['native'].astype('category').cat.codes\n",
    "df['city_encoded'] = df['city'].astype('category').cat.codes\n",
    "df['state_encoded'] = df['state'].astype('category').cat.codes\n",
    "\n",
    "# Features to use\n",
    "feature_cols = ['latitude_coordinate', 'longitude_coordinate', 'diameter_breast_height_CM',\n",
    "                'native_encoded', 'city_encoded', 'state_encoded']\n",
    "X = df[feature_cols]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit Nearest Neighbors model\n",
    "nn_model = NearestNeighbors(n_neighbors=50, algorithm='ball_tree')  # can tune n_neighbors\n",
    "nn_model.fit(X_scaled)\n",
    "\n",
    "# Prediction function\n",
    "def recommend_species(lat, lon, diameter_cm, native, city, state, top_n=5):\n",
    "    # Encode input\n",
    "    native_code = df['native'].astype('category').cat.categories.get_loc(native)\n",
    "    city_code = df['city'].astype('category').cat.categories.get_loc(city)\n",
    "    state_code = df['state'].astype('category').cat.categories.get_loc(state)\n",
    "\n",
    "    input_features = np.array([[lat, lon, diameter_cm, native_code, city_code, state_code]])\n",
    "    input_scaled = scaler.transform(input_features)\n",
    "\n",
    "    distances, indices = nn_model.kneighbors(input_scaled)\n",
    "\n",
    "    # Get common names or genera from neighbors\n",
    "    neighbors = df.iloc[indices[0]]\n",
    "    species_counts = Counter(neighbors['common_name'])  # or use 'genus'\n",
    "\n",
    "    # Top-N species\n",
    "    top_species = species_counts.most_common(top_n)\n",
    "    return top_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTXlCxtRjPBR",
    "outputId": "09c1b445-cd18-49d2-d11d-f50a11be61e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available native values: ['Native' 'Non-native']\n",
      "Available cities: ['Austin', 'Boston', 'Chicago', 'Dallas', 'Denver', 'Houston', 'New York', 'Phoenix', 'Portland', 'Seattle']\n",
      "Available states: ['AZ', 'CO', 'IL', 'MA', 'NY', 'OR', 'TX', 'WA']\n",
      "\n",
      "Recommended tree species:\n",
      "Pine (seen 10 times nearby)\n",
      "Poplar (seen 7 times nearby)\n",
      "Birch (seen 6 times nearby)\n",
      "Ash (seen 6 times nearby)\n",
      "Cherry (seen 5 times nearby)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Satyam\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example usage - using values that exist in our sample data\n",
    "print(\"Available native values:\", df['native'].unique())\n",
    "print(\"Available cities:\", sorted(df['city'].unique()))\n",
    "print(\"Available states:\", sorted(df['state'].unique()))\n",
    "\n",
    "# Use values that exist in our sample data\n",
    "recommendation = recommend_species(\n",
    "    lat=30.2672,  # Austin, TX coordinates\n",
    "    lon=-97.7431,\n",
    "    diameter_cm=25.0,\n",
    "    native='Native',  # Changed from 'naturally_occurring'\n",
    "    city='Austin',    # Changed from 'Louisville' \n",
    "    state='TX',       # Changed from 'Kentucky'\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "print(\"\\nRecommended tree species:\")\n",
    "for species, count in recommendation:\n",
    "    print(f\"{species} (seen {count} times nearby)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8R9F79Vfp_TP",
    "outputId": "fcf90db2-5659-4fa7-d547-8feee7d274a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1745.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Hit Rate: 0.3870\n",
      "Mean Reciprocal Rank: 0.1982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.387, 0.19821666666666668)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_recommender(X_scaled, df, model, top_k=5, sample_size=1000):\n",
    "    correct = 0\n",
    "    ranks = []\n",
    "\n",
    "    for i in tqdm(range(sample_size)):\n",
    "        x_query = X_scaled[i].reshape(1, -1)\n",
    "        distances, indices = model.kneighbors(x_query)\n",
    "\n",
    "        # exclude itself\n",
    "        neighbor_indices = [idx for idx in indices[0] if idx != i][:top_k]\n",
    "        true_species = df.iloc[i]['common_name']\n",
    "        neighbor_species = df.iloc[neighbor_indices]['common_name'].tolist()\n",
    "\n",
    "        if true_species in neighbor_species:\n",
    "            correct += 1\n",
    "            ranks.append(neighbor_species.index(true_species) + 1)\n",
    "        else:\n",
    "            ranks.append(0)\n",
    "\n",
    "    hit_rate = correct / sample_size\n",
    "    mean_rank = sum([1/r for r in ranks if r > 0]) / sample_size\n",
    "\n",
    "    print(f\"Top-{top_k} Hit Rate: {hit_rate:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank: {mean_rank:.4f}\")\n",
    "    return hit_rate, mean_rank\n",
    "\n",
    "# Run evaluation on a 1000-sample subset\n",
    "evaluate_recommender(X_scaled, df, nn_model, top_k=5, sample_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJdgr_CyqPo4",
    "outputId": "e5832e0b-ba7d-4356-d959-b29a63166afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scaler, model and data!\n"
     ]
    }
   ],
   "source": [
    "# Save scaler and model\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(nn_model, 'nn_model.joblib')\n",
    "\n",
    "# Also save the dataframe with encoded columns (needed for categories and lookup)\n",
    "df.to_pickle('tree_data.pkl')\n",
    "\n",
    "print(\"Saved scaler, model and data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GsThrmF1qrg0"
   },
   "outputs": [],
   "source": [
    "def get_common_locations_for_species(tree_name, top_n=10):\n",
    "    \"\"\"\n",
    "    Given a tree common name, return the top N most frequent locations.\n",
    "    \"\"\"\n",
    "    species_df = df[df['common_name'] == tree_name]\n",
    "    \n",
    "    if species_df.empty:\n",
    "        return f\"No records found for species: {tree_name}\"\n",
    "    \n",
    "    # You can group by city/state or full address\n",
    "    location_counts = species_df.groupby(['city', 'state']) \\\n",
    "                                .size().reset_index(name='count') \\\n",
    "                                .sort_values(by='count', ascending=False) \\\n",
    "                                .head(top_n)\n",
    "    \n",
    "    return location_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tree species in our data:\n",
      "common_name\n",
      "Oak       1053\n",
      "Cherry    1034\n",
      "Elm       1021\n",
      "Willow    1017\n",
      "Pine       996\n",
      "Ash        994\n",
      "Maple      985\n",
      "Birch      971\n",
      "Poplar     967\n",
      "Cedar      962\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top locations where 'Oak' is commonly found:\n",
      "        city state  count\n",
      "0     Austin    AZ     22\n",
      "34    Denver    IL     21\n",
      "59   Phoenix    MA     21\n",
      "4     Austin    NY     19\n",
      "15    Boston    WA     19\n",
      "21   Chicago    OR     19\n",
      "23   Chicago    WA     18\n",
      "3     Austin    MA     18\n",
      "51  New York    MA     18\n",
      "27    Dallas    MA     17\n",
      "\n",
      "Top locations where 'Pine' is commonly found:\n",
      "        city state  count\n",
      "33    Denver    CO     22\n",
      "67  Portland    MA     21\n",
      "52  New York    NY     20\n",
      "41   Houston    CO     19\n",
      "37    Denver    OR     18\n",
      "3     Austin    MA     17\n",
      "25    Dallas    CO     17\n",
      "36    Denver    NY     17\n",
      "19   Chicago    MA     17\n",
      "58   Phoenix    IL     17\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test the location function with tree species that exist in our sample data\n",
    "print(\"Available tree species in our data:\")\n",
    "print(df['common_name'].value_counts().head(10))\n",
    "\n",
    "# Test with Oak which should exist\n",
    "tree_name = 'Oak'\n",
    "top_locations = get_common_locations_for_species(tree_name)  # Fixed: removed df parameter\n",
    "print(f\"\\nTop locations where '{tree_name}' is commonly found:\")\n",
    "if isinstance(top_locations, str):  # Check if it's error message\n",
    "    print(top_locations)\n",
    "elif top_locations.empty:\n",
    "    print(f\"No records found for species: {tree_name}\")\n",
    "else:\n",
    "    print(top_locations)\n",
    "    \n",
    "# Test with another species\n",
    "tree_name = 'Pine'\n",
    "top_locations = get_common_locations_for_species(tree_name)  # Fixed: removed df parameter\n",
    "print(f\"\\nTop locations where '{tree_name}' is commonly found:\")\n",
    "if isinstance(top_locations, str):  # Check if it's error message\n",
    "    print(top_locations)\n",
    "elif top_locations.empty:\n",
    "    print(f\"No records found for species: {tree_name}\")\n",
    "else:\n",
    "    print(top_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
